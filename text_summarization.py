# -*- coding: utf-8 -*-
"""text_summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJ9W6YH78M9cp8jS_kjEZV303KAjepNK
"""





# load the english model
import spacy
nlp = spacy.load("en_core_web_sm") #small english lang model with core nlp components

text = "Data science and ai has a great career ahead"
doc = nlp(text)
doc

#since doc is content processed text, we can access each word as a token using token.tex
for token in doc:
  print(token.text)

#we can simply get the pos for each word by using token.pos_
for token in doc:
  print(f"{token} is a {token.pos_}")

#We can also get lemma, infact we can generate all these at once
for token in doc:
  print(token.text, token.pos_, token.lemma_)

# lets get another text and apply spacy func to it
nlp = spacy.load("en_core_web_sm")
document = nlp("Apple is looking at buying U>K startup for $1billion")
document

# generating tokens, pos, lemma
for token in document:
  print(token.text, token.pos_, token.lemma_)

for token in document:
  print(token.text, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)

for token in document:
  print(token.text, token.pos_)

# Another example

text = """There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.
An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.
Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[4] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured """

print(text)

# lets findout the list of stop words
#import spacy alreading imported
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

stopwords = list(STOP_WORDS)
stopwords

len(STOP_WORDS)

# lets create a spacy doc since we have en_core_web_sm already imported.
doc = nlp(text)
doc

# lets get tokens from the text
tokens = [token.text for token in doc]
print(tokens)

len(tokens)

# lets print out list of punctuation known to en_core_web_sm
print(punctuation)

# Determine the frequency of each word (token)
# 1. Word must not be punctuation
# 2. Word must not be stopword
# 3. We update word count as we loop through the tokens in the doc

word_frequencies = {}

for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in word_frequencies.keys():
        word_frequencies[word.text]=1
      else:
        word_frequencies[word.text] +=1

word_frequencies

len(word_frequencies)

#maxixum frequency

max_frequency = max(word_frequencies.values())
max_frequency

# To get normalized/weighted frequencies divide all frequencies with max_frequency (11)
for word in word_frequencies.keys():
  word_frequencies[word] = word_frequencies[word]/max_frequency

word_frequencies

# Get sentence tokens
sentence_tokens = [sent for sent in doc.sents]
sentence_tokens

# Number of sentences
len(sentence_tokens)

# To calculate the sentence score
sentence_scores = {}

for sent in sentence_tokens:
  for word in sent:
    if word.text.lower() in word_frequencies.keys():
      if sent not in sentence_scores.keys():
        sentence_scores[sent] = word_frequencies[word.text.lower()]
      else:
        sentence_scores[sent] += word_frequencies[word.text.lower()]

for k,v in sentence_scores.items():
  print(k,v)

# Lets import the heapq libray functions to work with largest values
from heapq import nlargest
select_length = int(len(sentence_tokens)*0.4)
select_length

# Lets select four sentences with max frequencies.
summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)
summary

sentence_scores

final_summary = [word.text for word in summary]
final_summary

